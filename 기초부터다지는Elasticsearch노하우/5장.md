# 5장. 클러스터 구축하기
ES 구축에 가장 중요한 환경 설정 파일
1. elasticsearch.yml
2. jvm.options

## elasticsearch.yml
ES 구성의 기본이 되는 환경 설정 파일

### Cluster 영역
- 클러스터 전체에 적용되는 설정
- cluster.name = my-application
    - 클러스터를 구성하기 위해선 모두 동일한 클러스터 이름을 사용해야 함
    - 클러스터 이름 변경 시 모든 노드을 재시작해야 함

### node 영역
- node.name = node-1
- 클러스터 내에서 유일해야 함
- node.name: ${HOSTNAME}
    - 노드 이름 변경 시 재시작 해야 함
- node.attr.rack: r1
    - 각 노드에 설정할 수 있는 커스텀 항목, 사용자 정의된 rack 값을 통해 HA 구성과 같이 샤드를 분배할 수 있음

### Path 영역
- 데이터와 로그의 저장 위치 설정
- path.data
    - /var/lib/elasticsearch : 노드가 가지고 있을 문서 저장, 세그먼트에 파일로 저장될 시의 파일 위치
- path.logs 
    - /var/log/elasticsearch : 로그 저장 경로

> 두 경로에 나뉘어서 세그먼트를 생성할 수 있음
> path.data: /var/lib/elasticsearch/data1, /var/lib/elasticsearch/data2
> 장점 : 여러개의 디스크를 가지고 있는 노드는 성능상 이점
> 단점 : 관리가 복잡해지고 두 개의 경로 중 하나에 문제가 발생했을 시 어떤 문서에 영향을 받는지 알기 어려움


### Memory 영역 
- ES 프로세스에 할당되는 메모리 영역을 어떻게 관리할 것인지 간략하게 설정이 가능함
- bootstap.memory_lock: true
    - 시스템이 스왑 메모리 영역을 사용하지 않도록 하는 설정
    - ES 는 스왑 메모리를 최대한 사용하지 않도록 권고함
    - 스왑 메모리를 사용하지 않으면 성능 상 이점이 있지만
    - OOM 에러를 일으켜서 노드의 장애로 이어질 수 있음
    - JVM 힙 메모리의 용량이 시스템 메모리의 용량의 절반 이상이 된다면 OOM 에러를 일으킬 수 있음
    - 이 설정을 사용하기 위해서는 OS 의 /etc/security/limits.conf 파일도 수정해야 함
    - sudo vi /etc/security/limits.conf
        - elasticsearch(계정명) soft memlock unlimited
        - elasticsearch(계정명) hard memlock unlimited
> 스왑 메모리 영역이란 ? 


### Network 영역
- ES 애플리케이션이 외부와 통신할 때 사용하게 된 ip 주소 설정
- 외부와의 통신뿐만 아니라 노드 간의 통신에도 네트워크 영역에서 설정한 값들을 바탕으로 동작함
- network.host: 192.168.0.1
    - ES 애플리케이션이 사용할 ip 주소
    - 내부용 : 127.0.0.1
    - 내부, 외부용 : 0.0.0.0
- network.bind_host: 클라이언트 요청을 처리하기 위한 ip
- network.publish_host: 클러스터 요청을 처리하기 위한 ip
    - 위 두 설정은 network.host 를 설정한다면 동일하게 설정되고,
    - network.host 를 설정하지 않는다면 아래처럼 설정된다
        -  network.bind_host: 0.0.0.0
        -  network.publish_host: 노드 자신의 ip
    - 노드 자신의 ip로 설정
        - localhost 라는 내부 도메인을 사용할 수 없음
        - localhost 를 사용하지 않아도 되지만 자동화를 통해 ES 를 관리할 때는 localhost라는 내부 도메인을 호출하는 것이 편함
    - 0.0.0.0 으로 설정
        - localhost 내부 도메인 사용 가능
        - ip 사용 가능
        - 클러스터 내 다른 노드들과 통신 시에도 0.0.0.0 을 사용하여 통신이 불가능해질 수 있음
결론
1. 클라이언트의 요청은 자신의 ip 뿐만 아니라 localhost 내부 도메인의 주소인 127.0.0.1도 받아야 함
2. 클러스터 내부에서 노드 간 통신은 자신의 ip를 사용해야 함

-> networks.bind_host : 0.0.0.0
-> networks.publish_host : 노드 자신의 ip


#### 0.0.0.0 을 사용하는 이유
- 내부 localhost와 노드의 ip 를 모두 사용할 수 있도록 연결해줌

### Discovery 영역
- 노드 간의 클러스터링을 위해 필요한 설정
- discovery.zen.ping.unicast.hosts: ["host1","host2"]
    - 클러스터링을 위한 다른 노드들의 정보 나열
    - ES 가 실행될 때 여기에 지정된 노드들로부터 클러스터의 이름, UUID, 현재 클러스터를 구성하고 있는 노드의 정보 등을 얻어옴
    - 상대적으로 변화가 적은 마스터 노드를 지정하는 것이 좋음
    - 전체 노드 지정하지 않아도 됨 ! !
    - 프로세스가 내려갔다가 올라가도 동일한 클러스터에 합류할 수 있음
- discovery.zen.minimum_master_nodes: 2
    - 클러스터를 구축하기 위해 필요한 최소한의 마스터 노드 대수
    - split brain을 방지하는 데 반드시 필요한 설정
    - total number of master-eligible nodes / 2 + 1 로 설정 권고 (정족수)

### Gateway 영역
- 클러스터 복구와 관련된 내용 포함
- gateway.recover_after_nodes: 3
    - gateway.recover_after_master_nodes: 3
    - gateway.recover_after_data_nodes: 3
    - 클러스터 내의 노드를 전부 재시작할 대 최소 몇 개의 노드가 정상적인 상태일 대 복구를 시작할 것인지 설정 
    - Full cluster restart
    - 클러스터 전체 장애가 발생했을 때 조금 더 안정적으로 클러스터를 복구할 수 있음
    - 적정한 수치란 ??
> split brain 이란 ? 
> 클러스터 내 마스터 노드가 n개가 있다. 
> 네트워크 장애가 발생하여 마스터 노드 간의 통신이 끊긴다면 서로 다른 마스터노드를 가진 두 개의 클러스터가 생길 수 있다.
> 만약 클러스터를 구성하기 위한 최소한의 마스터노드 개수가 더 크다면 split brain 이 발생하지 않을 수 있다.

### Various 영역
- action.destrctiove_requires_name: true
    - 클러스터에 저장되어 있는 인덱스를 _all이나 와일드카드로 제거할 수 없도록 막는 설정

### 노드의 역할 정의

| 노드역할 | 항목 | 기본 설정 값 |
| ------ | ------ |------ |
| 마스터노드 |node.master | TRUE|
| 데이터노드 | node.data |TRUE|
| 인제스트노드 | node.ingest |TRUE|
| 코디네이트노드 | 설정없음 |TRUE|

- 코디네이트 노드로만 설정하려면 위 항목들을 모두 false 로 설정하면 됨
- 전용 코디네이트 노드와 데이터 노드를 분리할 때에는 데이터 노드로부터 취합된 데이터를 담을 수 있을 정도로 충부한 리소스를 확보해야 한다
- 무조건 역할을 분리하는 것이 좋은게 아니라, 요청량이 적은 클러스터의 경우엔 역할을 중복하여 비용을 절약할 수 있다


##### 코디네이터 노드는 왜 필요할까 ?
- 코디네이터 노드는 사용자의 요청을 받아 이를 실제로 처리할 데이터 노드에 전달하고 각 데이터 노드로부터 받은 검색 결과도 하나로 취합해서 사용자에게 돌려주는 노드
- 사용자의 데이터 노드 중 한 대가 코디네이트 노드의 역할과 데이터 노드의 역할을 동시에 함으로써 해당 노드의 사용량이 높아지는 것을 방지하기 위함
- 데이터 노드 역할 : 검색 결과를 저장할 데이터 큐 필요
- 코디네이터 노드 역할 : 검색 결과를 취합하는 데이터 큐 필요
- 위 두 역할을 다 한다면 큐가 많이 필요해지고 힙 메모리 사용량이 증가됨 OOME 발생 가능성이 커짐, 특히 aggs 연산이 많은 경우는 역할을 분리해야 함 



## jvm.options 
- ES 는 자바 애플리케이션이기 때문에 힙메모리, GC 방식과 같은 JVM 관련 설정이 필요함
- GC 관련 옵션도 있으나, 성능에 많은 영향을 주기 때문에 많은 테스트를 하지 않았으면 기본 값으로 사용하는게 좋음
- 힙메모리 권장사항
    - 가능한 32GB를 넘지 않게 설정
    - 전체 메모리의 절반 정도를 힙 메모리에 설정
- default GC : CMS GC 
- xms1g
    - 힙 메모리 최소 크기
- xmx1g
    - 힙 메모리 최대 크기
    - 위 두 옵션은 같은 크기를 권고함, 중간에 메모리 요청이 추가로 일어나면 성능이 낮아지기 때문

##### 힙 메모리가 가능한 한 32GB를 넘지 않게 설정하도록 권고하는 이유 ? 
OOP (Ordinary Object Pointer)

##### 전체 메모리의 절반 정도를 힙 메모리로 할당하도록 권고하는 이유 ?
- ES 는 데이터를 세그먼트라는 물리적 파일에 저장함
- 물리 파일에 저장하므로 I/O 가 빈번히 발생할 수밖에 없음
- OS 에서는 I/O 로 인한 성능 저하를 막기 위해 메모리에 저장해두는 페이지 캐시 기법을 사용함
- 페이지 캐시는 애플리케이션이 사용하지 않는 미사용 메모리를 활용하여 동작함
- 때문에 ES 에서 페이지 캐시를 최대한 사용하기 위해 애플리케이션 메모리를 줄이는 것이 좋음
- ES 같이 I/O 작업이 빈번히 발생하는 경우 메모리를 페이지 캐시로 활용해서 I/O 작업이 모두 메모리에서 끝날 수 있도록 하는 것이 성능 확보에 도움이 됨


## 클러스터 구성하기
이 책에서는 마스터 전용 노드 3, 데이터 전용 노드 3대로 클러스터를 구성하였다.

노드의 증설이 필요할 경우 데이터 노드만 증설해주면 되기 때문
마스터 노드와 데이터 노드를 함께 증설하려면 discovery.zen.minimum_master_nodes 설정이 과반수 이상이 되도록 항상 신경써야 함

- 데이터 노드 한 대가 내려간다면 ? 
     - 해당 노드에 포함된 샤드들은 unassigned 상태에 빠지지만 레플리카 샤드가 프라이머리 샤드로 승격되면서 서비스는 지속됨
     - 기본으로 설정된 60초를 기다린 후에도 장애가 발생한 노드가 클러스터로 복구되지 않으면 unassigned 상태로 빠진 샤드들을 클러스터 내 다른 데이터 노드로 복제함

잘 구축된 ES 클러스터는 노드 장애에 큰 영향을 받지 않고 노드의 확장을 쉽게 진행할 수 있어서 데이터 저장공간이나 성능 부족에 대한 조치가 수월하다 !  !
이런 ES 를 운영할 수 있도록 계속해서 공부하자.
