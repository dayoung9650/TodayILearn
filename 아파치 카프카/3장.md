# 카프카 기본 개념
## 카프카 브로커
- 카프카 클라이언트와 데이터를 주고받기 위해 사용하는 주체
- 데이터를 분산저장하여 장애가 발생하더라도 안전하게 사용할 수 있도록 도와주는 애플리케이션
- 1 서버 = 1 브로커


## 데이터 저장 및 전송
- 프로듀서에서 데이터를 전달받으면 프로듀서가 요청한 토픽의 파티션에 데이터를 저장하고 컨슈머가 데이터를 요청하면 파티션에 저장된 데이터를 저정한다.
- 프로듀셔로부터 전달된 데이터는 파일 시스템에 저장된다
- 하나의 파티션에 하나의 디렉토리
- 토픽 이름과 파티션 번호의 조합으로 하위 디렉토리를 생성하여 데이터를 저장함

> 카프카가 파일시스템에 데이터를 저장(파일 입출력)하지만 느리지 않은 이유 ? 
> 페이지 캐시를 사용하여 디스크 입출력 속도를 높임
> 페이지 캐시 = OS 에서 파일 입출력의 성능 향상을 위해 만들어 놓은 메모리 영역
> 한 번 익은 파일의 내용은 메모리의 페이지 캐시 영역에 저장함 추후 동일한 파일의 접근이 일어나면 디스크에서 읽지 않고 메모리에서 직접 읽음
> 카프카 브로커를 실행하는데 힙 메모리 사이즈를 크게 설정할 필요가 없음

> ? OS 마다 페이지 캐시 방법이 다를텐데 OS 에 따라 성능이 달라질 수 있는걸까 ?

## 데이터 복제, 싱크
- 복제의 이유 : 클러스터 내 브로커 중 일부 장애가 발생하더라도 데이터 유실 없이 대응 가능
- 카프카의 데이터 복제는 파티션 단위
- 토픽을 생성할 때 파티션의 복제 개수도 같이 설정됨
- 직접 옵션을 선택하지 않으면 브로커에 설정된 옵션 값을 따라감 (최소 1 최대: 브로커 개수)
- 리더 : 실제 데이터 들은 곳, 프로듀셔 또는 컨슈머와 직접 통신하는 파티션
- 팔로워 : 복제 데이터 가지고 있는 파티션
- 복제 과정 : 팔로워 파티션들은 리더 파티션의 오프셋을 확인해서 현재 자신이 가지고 있는 오프셋과 차이나는 경우 리더 파티션으로부터 데이터 가져와서 자신의 파티션에 저장함
- 운영 시에는 데이터 종류마다 다른 복제 개수를 설정하고 토픽마다도 다른 복제 개수를 설정할 수 있음
- 데이터 유실되어도 괜찬 ? 처리 속도 중요 = 1,2 ..
- 유실 x : 3이상 

| ES   | 카프카 |     |
|------|-----|-----|
| 노드   | 브로커 |     |
| 샤드   | 파티션 |     |
| 마스터  | 리터  |     |
| 레플리카 | 팔로워 |     |
| 인덱스  | 토픽  |     |


## 컨트롤러
- 브로커 중 한 대가 컨트롤러 역할
- 브로커들의 상태를 체크하고 리더 파티션을 재분배 함
- 지속적으로 데이터를 처리해야 하므로 브로커의 상태가 비정상적이라면 빠르게 클러스터를 빼내는게 좋음

## 데이터 삭제
> 1GB 가 쌓이는 동안 세그먼트 오픈 -> 닫힘 -> 이후 log.retention.* 이 지난 시점에서 데이터 삭제됨

> ? 데이터가 한 번도 안가져가져도 삭제되나 ?!?!

- 컨슈머가 데이터를 가져가더라도 데이터를 삭제하지는 않음
- 오직 브로커만이 데이터를 삭제
- 데이터 삭제는 파일 단위로 이루어짐 - 로그 세그먼트
- 세그먼트는 데이터가 쌓이는 동안 파일 시스템에 열려있음 log.segment.bytes, ms(기본 값 1GB) 옵션에 값이 설정되면 세그먼트 파일이 닫힘
- 해당 값을 너무 작게 설정해두면 세그먼트 파일 너무 자주 여닫음 -> 부하
- 닫힌 세그먼트 파일은 log.retention.bytes, ms 옵션 값을 넘으면 삭제됨
- 닫힌 세그먼트 파일을 체크하는 간격 : log.retention.check.interval.ms 

## 컨슈머 오프셋 저장
- 컨슈머 그룹은 토픽이 특정 파티션으로부터 데이터를 가져가서 처리하고 이 파티션의 어느 레코드까지 가져갔는지 확인하기 위해 오프셋을 커밋
- _consumer_offsets

## 코디네이터
- 컨슈머 그룹의 상태를 체크하고 파티션을 컨슈머와 매칭되도록 분배하는 역할 (리밸런스)


## 주키퍼의 역할
- 카프카의 메타 데이터를 관리하는데 사용
- 분산 애플리케이션 사용 시 분산 애플리케이션 관리를 위해 안정적인 코디네이터 애플리케이션이 필요하
- 주키퍼 :
  - 분산 애플리케이션 코디네이션 시스템 
  - 분산 되어 있는 매플리케이션 정보 중앙 집중 구성 관리 네이밍 동기화함
- 카프카 클러스터로 묶인 브로커들은 동일한 경로의 주키퍼 경로로 설ㄴ언해야 같은 카프카 브로커 묶음이 됨


## 토픽과 파티션
- 토픽은 카프카에서 데이터를 구분하기 위해 사용하는 단위 
- 1개 이상의 파티션을 소유하고 있음
- 파티션은 카프카의 병렬 처리의 핵심으로써 그룹으로 묶인 컨슈머들이 레코드를 병렬로 처리 할 수 있도록 매칭된다 
- 일반적인 자료구조와는 달리 카프카는 데이터를 삭제하지 ㅇ낳음 
- 파티션의 레코드는 컨슈머가 가져가는 것과 별도로 관리됨
- 토픽의 레코드는 다양한 목적을 가진 여러 컨슈머 그룹들이 토픽의 데이터를 여러 번 가져갈 수 있다
> ? 컨슈머의 처리량이 한정된 상태에서 많은 레코드를 병렬로 처리하는 가장 좋은 바업은 컨슈머의 개수를 늘려 스케일아웃 하는 것이다.
> ? 파티션이랑 컨슈머를 매칭하는 개념이 이해가 안감 


### 토픽 이름
- 대소문자 구분함, . , _ 연속 불가
- 토픽 이름 변경 안됨
- 처음부터 룰을 만들어서 규칙을 잘 따르자 !


## 레코드 
- 타임프탬프, 메시지 키, 메시지 값, 오프셋, 헤더
- 브로커에 한번 적재된 레코드는 수정할 수 없고 로그 리텐션 기간 또는 용량에 따라서만 삭제됨 
### 타임스탬프
- 해당 레코드가 생성된 시점의 유닉스 타임이지만, 프로슈서가 변경가능하고 브로커에 따라 브로커 적재 시간일 수 있음
### 메시지 키 
- 어떤 파티션에 저장할지 결정, 동일한 메시지키라면 동일 파티션에 저장됨 
- 파티션 개수 달라지면 매칭이 달라짐
- 메시지키 null 일 경우 프로듀서 기본 설정 파티셔너에 따라 분배됨
- 직렬화되어 브로커 전달

### 메시지 값
- 실질적 처리 데이터
- 직렬화되어 브로커로 전달
- 컨슈머가 동일한 직렬화 방법으로 역직렬화 해야 함

### 오프셋
- 0 이상의 숫자
- 레코드의 오프셋은 직접 지정할 수 없고 브로커에 저장될 때 이전에 전송된 레코드의 오프센 + 1 값으로 생성
- 컨슈머가 데이터를 가져갈 때 사용됨


## 카프카 클라이언트
> 카프카 클러스터에 명령을 내리거나 데이터를 송수신하기 위해 카프카 클라이언트 라이브러리는 카프카 프로듀서 컨슈머 어드민 클라이언트를 제공



### 프로듀서 API
- 카프카에 필요한 데이터를 선언하고 브로커의 특정 토픽의 파티션에 전송한다.
- 프로듀서는 데이터를 전송할 때 리더 파티션을 가지고 있는 카프카 브로커와 직접 통신함


## 카프카 커넥트
- 카프카 오픈소스 포함된 툴 중 하나, 데이터 파이프라인 생성 시 반복 작업을 줄이고 효율적인 전송을 이루기 위한 애플리케이션
- 반복적인 파이프라인 생성 작업이 있을 때는 매번 프로듀서 컨슈머 애플리케이션을 개발 및 배포 운영 해야 하기 때문에 비효율적
- 커넥트는 특정 작업 형태를 템플릿으로 만들어 놓은 커넥터를 실행함으로써 반복 작업을 줄임
- 카프카 2.6 에 포함된 커넥트는 미러메이커2, 커넥터, 파일 커넥터, 소스 커넥터를 기본 플러그인으로 제공
- 추가 커넥터는 플러그인 형태로 커넥터 jar 파일 추가하여 사용 가능
- 커넥터 플러그인을 추가하고 싶다면 직접 커넥터 플러그인을 만들거나 인터넷의 커넥터 플러그인 가져다 쓸 수 있음
### 커넥터 종류
- 소스 커넥터 : 프로듀서 / 데이터를 카프카 토픽으로 전송하는 역할
- 싱크 커넥터 : 컨슈머 / 토픽의 데이터를 파일로 저장하는 컨슈머 역할

### 커넥터와 태스크
- 커넥터에 커넥터 생성 명령을 내리면 커넥터와 하위 태스크들을 생성
- 커넥터들은 태스크를 관리함
- 태스크는 커넥터에 종속되는 개념으로 실질적인 데이터 처리함
- transform : 메시지 단위로 데이터를 간단하게 변환하기 위한 용도로 사용
- converter : 스키마를 변경하도록 도와줌, jsonConverter, StringConverter 등 커스텀 컨버터도 지원

### 커넥터 실행 방법
1. 단일모드 커넥트
   - 프로세스 1개만 실행
   - 고가용성 구성되지 않아 단일 장애점이 될 수 있음
   - 중요도 낮은 애플리케이션 처리
2. 분산모드 커넥트
   - 2대 이상의 서버에서 클러스터 형태로 운영
   - 한개의 커넥트가 이슈 발생되더라도 클러스터 내 다른 커넥트로 운영 가능
   - 처리량의 변화에도 유연하게 대응 가능
   - 2대 이상의 프로세스가 1개의 그룹으로 묶여서 운영됨
   - 분산 모드 커넥터가 실행되고 난 후에는 rest api 로 커넥트의 상태, 생성 조회 수정 중단 명령을 날릴 수 있다.
> 커넥트에서 사용할 수 있는 플러그인 조회
```curl
curl -X GET http://localhost:8083/connector-plugiins
```
- 카프카 2.5.0 에서는 FileStreamSinkConnector, FileStreamSourceConnector, MirrorCheckpointConnector, MirrorHeartbeatConnector, MirrorSourceConnector 5가지 기본 지원


### 카프카 커넥트 Rest API
- 현재 실행 중인 커넥트의 커넥터 플러그인 종류 및 태스크 상태, 커넥터 상태 조회할 수 있음


## 소스 커넥터
- 소스 애플리케이션으로부터 데이터를 가져와 토픽으로 넣는 역할을 함
- 오픈소스 소스 커넥터를 사용해도 되지만 SourceConnector 와 SourceTask 클래스를 사용하여 직접 소스 커넥터 구현
- 직접 구현한 소스 커넥터를 빌드하여 jar 파일로 만들어서 커넥트 실행 시 플러그인으로 추가하여 사용할 수 있음
- connect-api 라이브러리를 추가해야 함
- SourceConnector/ SourceTask 2개 필요
- SourceConnector
  - 설정파일을 초기화하고 어떤 태스크 클래스를 사용할 것인지 정의하는데 사용
  - MongoDbSourceConnector extends SourceConnector 
- SourceTask
  - 실제로 데이터를 다루는 클래스
  - 소스 애플리케이션 또는 소스 파일로부터 데이터를 가져와서 토픽으로 데이터를 보내는 역할 수행
  - 토픽애서 사용하는 오프셋이 아닌 자체적으로 사용하는 오프셋을 사용함
  - 소스 애플리케이션 또는 소스 파일을 어디까지 읽었는지 저장하는 역할 
  - 데이터를 중복해서 토픽으로 보내는 것을 방지할 수 있음

## 싱크 커넥터
- 토픽의 데이터를 타킷 애플리케이션 또는 타킷 파일로 저장하는 역할
- SinkConnector, SinkTask 클래스 사용하여 직접 커넥터 구현할 수 있음
- coonector-api 라이브러리를 추가해야 함
- SinkConnector
  - 태스크를 실행하기 전 사용자로부터 입력받은 설정값을 초기화하고 어떤 태스크 클래스를 사용할 것인지 정의
  - 실질적인 데이터 처리 로직X
- SinkTask
  - 커넥트에서 컨슈머 역할을 하고 데이터를 저장
> 싱크 커넥터는 토픽을 옵션값으로 받지 않음. 커넥트를 통해 커넥터 실행 시 기본값으로 받아야 하기 때문


여기까지 읽고 느낀 점 : 커넥터 내 커넥트들간의 종속성이 너무 높지 않나 ?

# 카프카 미러메이커2
> 서로 다른 두 개의 카프카 클러스터 간에 토픽을 복제하는 애플리케이션
- 목적 : 프로듀서와 컨슈머를 사용해서 직접 미러링하는 애플리케이션을 만들면 되지만 토픽의 모든 것을 복제할 필요성이 있기 때문에 사용
- 토픽에 있는 레코드는 고유한 메시지 키, 값, 파티션을 가진다
- 프로듀서와 컨슈머를 사용해서 2개ㅑ의 서로 다른 클러스터에 토픽의 데이터를 완전히 동일하게 옮기는 것은 어려움
- 미러메이커1 은 복제전 데이터와 복제 후 데이터의 파티션 정보가 달랐음
- 복제하는 토픽이 달라지면 수정하기 위해 미러메이커 애플리케이션을 재시작해야 했음
- exactly once delivery 를  보장하지 못함 -> 유실 또는 중복 가능성
- 토픽의 데이터 뿐만 아니라 토픽 설정까지도 복제하여 파티션의 변화 토픽 설정 값의 변화도 동기화함
- 파티션 설정도 복제
- default 5초마다 토픽의 설정값을 확인하고 동기화함

## 미러메이커2를 활용한 지리적 복제
> 카프카 클러스터 단위의 활용도를 높일 수 있음
> 단방향, 양방향 복제 기능, ACL 복제, 새 토픽 자동 감지 등의 기능 유용

### 액티브 - 스탠바이 클러스터 운영
- 재해 복구를 위해 임시 카프카 클러스터를 하나 더 구성하는 경우 active-standby 로 운영할 수 있음
- 장애 발생 시 서비스 완전 중단을 막을 수 있음
- 다만 복제 랙이 발생할 수는 있음

### 액티브 - 액티브 클러스터 운영
- 글로벌 서비스를 운영할 경우 통신 지연을 최소화하기 위해 2개 이상의 클러스터를 두고 서로 데이터를 미러링하면서 사용할 수 있음
- 물리적으로 떨어져 있는 두 명의 유저의 데이터를 저장하고 사용하는 방법으로 각 지역마다 클러스터를 두고 필요한 데이터만 복제
- 물리적으로 떨어져 있는 데이터들만 미러링하면 됨 (영국: 한국 데이터 가져오기 / 한국: 영국 데이터 가져오기)

### 허브 앤 스포크 클러스터 운영
- 각 팀의 카프카 클러스터의 데이터를 한 개의 카프카 클러스터에 모아 데이터 레이크로 사용