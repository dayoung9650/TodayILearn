# 아파치 카프카 들어가며 ! 
## 카프카의 탄생
- 링크드인에서 파편화된 데이터 수집 및 분배 아키텍처 운영하는데 어려워서 만들어짐 
- 기존 : 데이터를 생성하는 소스 애플리케이션과 데이터 최종 적재되는 타킷 애플리케이션 연경헤애 힘 -> 아키텍처 거대해지고 소스 애플리케이션이 많아지면서 문제가 생김
- 링크드인의 내부 흐름을 개선하기 위해 개발함
- 각각의 애플리케이션끼리 연결하여 데이터를 처리하는 것이 아니라 중앙 집중화함
- 소스 애플리케이션, 타깃 애플리케이션 사이의 결합도를 낮춤
- 소스 애플리케이션에서 바로 카프카에 넣으면 됨
- FIFO
- 데이터 포맷 제한 없음
- 직렬화, 역직렬화 ByteArray 로 통신 
- 최소 3대 이상의 서버 (브로커)에서 분산 운영함
- 배치 전송 -> 낮은 지연과 높은 데이터 처리량
- 대량의 데이터를 안전하고 빠르게 처리하는 강점


## 빅데이터 파이프라인에서 카프카의 역할
- 빅데이터를 저장하고 활용하기 위해서는 일단 생성되는 데이터를 모두 모으는 것이 중요 -> 데애터 레이크
- 데이터 웨어하우스 : 필터링되거나, 패키지화된 데이터
- 데이터 레이크 : 서비스로부터 수집 가능한 모든 데이터를 모음

### 카프카의 특징
1. 높은 처리량
    - 네트워크 비용을 줄이기 위해 벌크로 처리함
    - 벌크의 크기를 결정하는 것이 중요할듯
    - 파티션 단위를 동일 목적의 데이터를 여러 파티션에 분배하고 데이터를 병렬처리할 수 있음
2. 확장성
   - 스케일 아웃
   - 무중단 운영 지원
3. 영속성
   - 카프카는 데이터를 메모리에 저장하지 않고 파일시스템에 저장함
   - 운영체제 레벨에서 파일시스템을 최대한 활용함
   - I/O 성능 향상을 위해 페이지 캐시 영역을 메모리에 따로 생성하여 사용함
   - 디스크 기반의 파일 시스템을 활용한 덕분에 브로커 애플리케이션이 장애 발생으로 인해 급작스럽게 종료되더라도 프로세스 재시작하여 안전하게 다시 시작 가능
4. 고가용성
   - 3대 이상의 서버들로 구성함
   - 복제를 통해 고가용성 가능

> 카프카 클러스터를 3대 이상의 브로커로 구성해야 하는 이유
>  - 만약 노드 한 대가 죽는다면 나머지 두 대에서 죽은 노드에 있던 데이터를 조회할 수 있어야 함
> min.insync.replicas 옵션을 2로 설정하면 최소 2개 이상의 브로커에 데이터가 복제됨

## 람다 아키텍처
- 서빙 레이어/ 배치 레이어 / 스피드 레이어를 구분
- 서빙 레이어: 가공된 데이터를 애플리케이션에서 사용할 수 있도록 제공
- 배치 레이어: 배치 데이터를 모아 특정 시간 타이밍마다 일괄 처리함 
- 스피드 레이어: 서비스에서 생성되는 원천 데이터를 실시간으로 분석함 (카프카 여기에 속함, 실시간 데이터를 짧은 지연시간으로 처리 분석할 수 있음)

## 카파 아키텍처
- 배치 레이어를 제외하고 모든 데이터를 스피드 레이어에 넣음
- 스피드 레이어로 사용되는 카프카에 분석과 프로세싱을 완료한 거대한 용량의 데이터를 오랜 기간 저장하고 사용할 수 있다면 서빙 레이어는 제거되어도 됨
- 카프카의 역할이 커지면서 스피드 레이어만으로 파이프라인을 유지할 수 있음

